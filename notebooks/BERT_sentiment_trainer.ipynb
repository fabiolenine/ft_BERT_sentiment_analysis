{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3489d163",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a0c3d",
   "metadata": {},
   "source": [
    "This script fine-tunes a pre-trained BERT model for sentiment analysis using the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fa5ae",
   "metadata": {},
   "source": [
    "### Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814ca94",
   "metadata": {},
   "source": [
    "### Verifying GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU disponível: {torch.cuda.get_device_name(0)}\")\n",
    "    # Limpa o cache da GPU\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"GPU não disponível. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heading the BERT model and tokenizer from Hugging Face Transformers\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Data loading library\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abfee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bed5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcff94",
   "metadata": {},
   "source": [
    "### Track Training and Test with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"../logs/runs/sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60aa61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"../logs/runs/sentiment_analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60de77",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to be used for training and evaluation\n",
    "# This dataset is a collection of B2W reviews, which will be used for sentiment analysis\n",
    "# Note: Ensure that the dataset is available in the Hugging Face datasets library\n",
    "DATASET_HF = \"ruanchaves/b2w-reviews01\"\n",
    "\n",
    "dataset = load_dataset(DATASET_HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the dataset structure and contents\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f6e7b",
   "metadata": {},
   "source": [
    "## Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e375a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Preprocessamento: conversão para minúsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenização com padding, truncamento e máscara de atenção\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850712e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentTrainer:\n",
    "    def __init__(self, model_name='neuralmind/bert-base-portuguese-cased', max_length=128, num_labels=2\n",
    "                 , dataset_name=\"ruanchaves/b2w-reviews01\"):       \n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.num_labels = num_labels\n",
    "        self.dataset_name = dataset_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Inicializar tokenizer e modelo, carregando todos os pesos pré-treinados\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=num_labels\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Métricas de treinamento\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "        print(f\"Modelo carregado: {model_name}\")\n",
    "        print(f\"Dispositivo: {self.device}\")\n",
    "        print(f\"Dataset configurado: {self.dataset_name}\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Carrega e processa o dataset B2W Reviews\"\"\"\n",
    "        print(f\"Carregando dataset: {self.dataset_name}...\")\n",
    "        \n",
    "        # Carregar dataset do Hugging Face\n",
    "        dataset = load_dataset(self.dataset_name)\n",
    "        \n",
    "        # Converter para DataFrame (só tem split train)\n",
    "        all_data = pd.DataFrame(dataset['train'])\n",
    "        \n",
    "        # Assumindo que as colunas são 'text' e 'label'\n",
    "        texts = all_data['review_text'].tolist()\n",
    "        labels = all_data['overall_rating'].tolist()\n",
    "        #recommend_to_a_friend\n",
    "        \n",
    "        # Converter ratings para classificação binária (1-2: negativo=0, 3-5: positivo=1)\n",
    "        binary_labels = [1 if rating >= 3 else 0 for rating in labels]\n",
    "        \n",
    "        print(f\"Total de amostras: {len(texts)}\")\n",
    "        print(f\"Distribuição de classes: {pd.Series(binary_labels).value_counts()}\")\n",
    "        \n",
    "        return texts, binary_labels\n",
    "    \n",
    "    \n",
    "    def prepare_data(self, texts, labels, test_size=0.2, val_size=0.1, \n",
    "                     seed_random_state=None):\n",
    "        \"\"\"Prepara os dados para treinamento\"\"\"\n",
    "        # Primeiro, separa o teste\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            texts, labels, test_size=test_size, random_state=seed_random_state, stratify=labels)\n",
    "    \n",
    "        # Agora calcula o percentual real de validação sobre o restante\n",
    "        val_relative = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_relative, random_state=seed_random_state, stratify=y_temp)\n",
    "\n",
    "        # Criar datasets\n",
    "        train_dataset = SentimentDataset(X_train, y_train, self.tokenizer, self.max_length)\n",
    "        val_dataset = SentimentDataset(X_val, y_val, self.tokenizer, self.max_length)\n",
    "        test_dataset = SentimentDataset(X_test, y_test, self.tokenizer, self.max_length)\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, batch_size=16, epochs=3, learning_rate=2e-5):\n",
    "        \"\"\"Treina o modelo BERT\"\"\"\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Otimizador e scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        print(f\"Iniciando treinamento por {epochs} épocas...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nÉpoca {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            # Treinamento\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            train_predictions = []\n",
    "            train_true_labels = []\n",
    "            \n",
    "            train_pbar = tqdm(train_loader, desc=\"Treinamento\")\n",
    "            for batch in train_pbar:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_train_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Métricas\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                train_predictions.extend(predictions.cpu().numpy())\n",
    "                train_true_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                train_pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
    "            \n",
    "            # Validação\n",
    "            val_loss, val_accuracy, val_metrics = self.evaluate(val_loader)\n",
    "            \n",
    "            # Salvar métricas\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            self.val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Registrar métricas no TensorBoard\n",
    "            writer.add_scalar(\"Loss/Treino\", avg_train_loss, epoch + 1)\n",
    "            writer.add_scalar(\"Loss/Validação\", val_loss, epoch + 1)\n",
    "            writer.add_scalar(\"Acurácia/Treino\", train_accuracy, epoch + 1)\n",
    "            writer.add_scalar(\"Acurácia/Validação\", val_accuracy, epoch + 1)\n",
    "            \n",
    "            print(f\"Loss de treino: {avg_train_loss:.4f}\")\n",
    "            print(f\"Acurácia de treino: {train_accuracy:.4f}\")\n",
    "            print(f\"Loss de validação: {val_loss:.4f}\")\n",
    "            print(f\"Acurácia de validação: {val_accuracy:.4f}\")\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"Avalia o modelo\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        logits_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Avaliação\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                batch_predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                predictions.extend(batch_predictions.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                logits_list.extend(torch.softmax(logits, dim=-1).cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            true_labels, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # AUC-ROC\n",
    "        probabilities = np.array(logits_list)[:, 1]  # Probabilidade da classe positiva\n",
    "        auc_roc = roc_auc_score(true_labels, probabilities)\n",
    "        \n",
    "        metrics = {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc_roc': auc_roc\n",
    "        }\n",
    "        \n",
    "        return avg_loss, accuracy, metrics\n",
    "    \n",
    "    def save_model(self, save_path, metrics, hyperparameters):\n",
    "        \"\"\"Salva o modelo e metadados\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Salvar modelo e tokenizer\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        # Salvar métricas e hiperparâmetros\n",
    "        metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_name': self.model_name,\n",
    "            'max_length': self.max_length,\n",
    "            'hyperparameters': hyperparameters,\n",
    "            'metrics': metrics,\n",
    "            'device': str(self.device)\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_path, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Modelo salvo em: {save_path}\")\n",
    "    \n",
    "    def load_model(self, load_path):\n",
    "        \"\"\"Carrega modelo salvo\"\"\"\n",
    "        self.model = BertForSequenceClassification.from_pretrained(load_path).to(self.device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(load_path)\n",
    "        \n",
    "        with open(os.path.join(load_path, 'metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def plot_training_history(self, save_path=None):\n",
    "        \"\"\"Plota histórico de treinamento\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        axes[0].plot(self.train_losses, label='Treino')\n",
    "        axes[0].plot(self.val_losses, label='Validação')\n",
    "        axes[0].set_title('Loss durante o Treinamento')\n",
    "        axes[0].set_xlabel('Época')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Acurácia\n",
    "        axes[1].plot(self.train_accuracies, label='Treino')\n",
    "        axes[1].plot(self.val_accuracies, label='Validação')\n",
    "        axes[1].set_title('Acurácia durante o Treinamento')\n",
    "        axes[1].set_xlabel('Época')\n",
    "        axes[1].set_ylabel('Acurácia')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(os.path.join(save_path, 'training_history.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVersionManager:\n",
    "    def __init__(self, base_path='../models'):\n",
    "        self.base_path = base_path\n",
    "        self.experiments_path = os.path.join(base_path, 'experiments')\n",
    "        self.production_path = os.path.join(base_path, 'production')\n",
    "        \n",
    "        os.makedirs(self.experiments_path, exist_ok=True)\n",
    "        os.makedirs(self.production_path, exist_ok=True)\n",
    "    \n",
    "    def save_experiment(self, trainer, metrics, hyperparameters, experiment_name=None):\n",
    "        \"\"\"Salva experimento com timestamp\"\"\"\n",
    "        if experiment_name is None:\n",
    "            experiment_name = f\"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        experiment_path = os.path.join(self.experiments_path, experiment_name)\n",
    "        trainer.save_model(experiment_path, metrics, hyperparameters)\n",
    "        trainer.plot_training_history(experiment_path)\n",
    "        \n",
    "        return experiment_path\n",
    "    \n",
    "    def promote_to_production(self, experiment_path, model_name='best_model'):\n",
    "        \"\"\"Promove modelo para produção se melhor que o atual\"\"\"\n",
    "        production_model_path = os.path.join(self.production_path, model_name)\n",
    "        \n",
    "        # Carregar métricas do experimento\n",
    "        with open(os.path.join(experiment_path, 'metadata.json'), 'r') as f:\n",
    "            new_metadata = json.load(f)\n",
    "        \n",
    "        # Verificar se existe modelo em produção\n",
    "        current_metadata_path = os.path.join(production_model_path, 'metadata.json')\n",
    "        \n",
    "        should_promote = True\n",
    "        if os.path.exists(current_metadata_path):\n",
    "            with open(current_metadata_path, 'r') as f:\n",
    "                current_metadata = json.load(f)\n",
    "            \n",
    "            # Comparar F1-score (ou outra métrica principal)\n",
    "            current_f1 = current_metadata['metrics']['f1']\n",
    "            new_f1 = new_metadata['metrics']['f1']\n",
    "            \n",
    "            if new_f1 <= current_f1:\n",
    "                should_promote = False\n",
    "                print(f\"Modelo atual (F1: {current_f1:.4f}) é melhor que o novo (F1: {new_f1:.4f})\")\n",
    "        \n",
    "        if should_promote:\n",
    "            # Copiar modelo para produção\n",
    "            import shutil\n",
    "            if os.path.exists(production_model_path):\n",
    "                shutil.rmtree(production_model_path)\n",
    "            shutil.copytree(experiment_path, production_model_path)\n",
    "            \n",
    "            print(f\"Modelo promovido para produção! F1-Score: {new_metadata['metrics']['f1']:.4f}\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def list_experiments(self):\n",
    "        \"\"\"Lista todos os experimentos\"\"\"\n",
    "        experiments = []\n",
    "        for exp_name in os.listdir(self.experiments_path):\n",
    "            exp_path = os.path.join(self.experiments_path, exp_name)\n",
    "            metadata_path = os.path.join(exp_path, 'metadata.json')\n",
    "            \n",
    "            if os.path.exists(metadata_path):\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                experiments.append({\n",
    "                    'name': exp_name,\n",
    "                    'timestamp': metadata['timestamp'],\n",
    "                    'metrics': metadata['metrics']\n",
    "                })\n",
    "        \n",
    "        return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22704a58",
   "metadata": {},
   "source": [
    "## Pipeline of Fine-tuning BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3dd60",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e6365",
   "metadata": {},
   "source": [
    "#### Data Catalogy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca82025",
   "metadata": {},
   "source": [
    "**Data Fields**\n",
    "\n",
    "\n",
    "| Column\t                | Description                                                                           |\n",
    "|---------------------------|---------------------------------------------------------------------------------------|\n",
    "| submission_date           | The date and time when the review was submitted. Format: \"%Y-%m-%d %H:%M:%S\".         |\n",
    "| reviewer_id               | A unique identifier for the reviewer.                                                 |\n",
    "| product_id\t            | A unique identifier for the product being reviewed.                                   |\n",
    "| product_name\t            | The name of the product being reviewed.                                               |\n",
    "| product_brand\t            | The brand of the product being reviewed.                                              |\n",
    "| site_category_lv1         | The highest level category for the product on the site where the review is submitted. |\n",
    "| site_category_lv2\t        | The second level category for the product on the site where the review is submitted.  |\n",
    "| review_title\t            | The title of the review.                                                              |\n",
    "| **overall_rating**        | The overall star rating given by the reviewer on a scale of 1 to 5.                   |\n",
    "| **recommend_to_a_friend**\t| Whether or not the reviewer would recommend the product to a friend (Yes/No).         |\n",
    "| **review_text**           | The full text of the review.                                                          |\n",
    "| reviewer_birth_year\t    | The birth year of the reviewer.                                                       |\n",
    "| reviewer_gender\t        | The gender of the reviewer (F/M).                                                     |\n",
    "| reviewer_state\t        | The Brazilian state of the reviewer (e.g., RJ).                                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c61c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the dataset into a DataFrame for further analysis\n",
    "dataset_df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa873077",
   "metadata": {},
   "source": [
    "#### **Target:** Overall_rating vs Recommend_to_a_friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddb535",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Gerar matriz de cruzamento (crosstab) entre 'overall_rating' e 'recommend_to_a_friend'\n",
    "cross_tab = pd.crosstab(dataset_df['overall_rating'], dataset_df['recommend_to_a_friend'], normalize='index') * 100\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cross_tab, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar_kws={'format': '%.0f%%'})\n",
    "plt.title('Matriz de Cruzamento: Nota vs. Recomendaria a um Amigo (%)')\n",
    "plt.xlabel('Recomendaria a um Amigo')\n",
    "plt.ylabel('Nota (overall_rating)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1791b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_no = dataset_df[dataset_df['recommend_to_a_friend'] == 'No']['overall_rating']\n",
    "x_yes = dataset_df[dataset_df['recommend_to_a_friend'] == 'Yes']['overall_rating']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x_no, name='Recomendaria a um Amigo: Não', marker_color='red'))\n",
    "fig.add_trace(go.Histogram(x=x_yes, name='Recomendaria a um Amigo: Sim', marker_color='blue'))\n",
    "\n",
    "# The two histograms are drawn on top of another\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9957e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o percentual de 'Yes' e 'No' na coluna 'recommend_to_a_friend'\n",
    "percentual = dataset_df['recommend_to_a_friend'].value_counts(normalize=True) * 100\n",
    "print(percentual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar valores de overall_rating como positivo (3 a 5) e negativo (1 a 2)\n",
    "dataset_df['sentiment'] = dataset_df['overall_rating'].apply(lambda x: 'positivo' if x >= 3 else 'negativo')\n",
    "\n",
    "# Calcular o percentual de positivos e negativos\n",
    "percentual_sentiment = dataset_df['sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(percentual_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e300b",
   "metadata": {},
   "source": [
    "#### Discovery MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['review_text'] = dataset_df['review_text'].str.strip()\n",
    "dataset_df['review_text_length'] = dataset_df['review_text'].apply(lambda x: len(x) if pd.notnull(x) else 0)\n",
    "\n",
    "# Gerar o gráfico de distribuição com agrupamento de 100 em 100\n",
    "plt.figure(figsize=(19, 8))\n",
    "sns.histplot(dataset_df['review_text_length'], binwidth=50, kde=True, color='blue')\n",
    "plt.title('Distribuição do Número de Caracteres por Review (Agrupamento de 50)')\n",
    "plt.xlabel('Número de Caracteres')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o gráfico de distribuição com agrupamento de 500 em 500 e frequência em percentil\n",
    "plt.figure(figsize=(19, 8))\n",
    "\n",
    "# Calcular a frequência relativa (percentil)\n",
    "total_count = len(dataset_df['review_text_length'])\n",
    "sns.histplot(\n",
    "    dataset_df['review_text_length'], \n",
    "    binwidth=500, \n",
    "    kde=False, \n",
    "    color='blue', \n",
    "    stat='percent'\n",
    ")\n",
    "\n",
    "plt.title('Distribuição do Número de Caracteres por Review (Agrupamento de 500)')\n",
    "plt.xlabel('Número de Caracteres')\n",
    "plt.ylabel('Frequência (%)')\n",
    "# Definir granularidade do eixo Y de 5 em 5%\n",
    "plt.yticks(np.arange(0, 101, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967700e",
   "metadata": {},
   "source": [
    "**Constatamos:** que ao observar o gráfico de distribuição, aproximadamente 95% dos registros estão com até 500 caracteres, o que ultrapassa estão dentro dos 5% restante. Desta forma, será interessante utilizar o MAX_LENGTH = 512, para que o contexto dos ~95% dos registros e os demais serão truncados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86f9ba",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros para iniciar a classe do modelo BERT\n",
    "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
    "NUM_LABELS = 2\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Parametros de treinamento\n",
    "BATCH_SIZE = 16  # Ajustado para 16 para evitar problemas de memória, apenas temos 8192 MiB de GPU\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "    \n",
    "# Hiperparâmetros\n",
    "hyperparameters = {\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar trainer fazendo a carga do modelo BERT\n",
    "trainer = BERTSentimentTrainer(model_name=MODEL_NAME, max_length=MAX_LENGTH, num_labels=NUM_LABELS\n",
    "                               ,dataset_name=DATASET_HF)\n",
    "\n",
    "# Inicializar o gerenciador de versões\n",
    "version_manager = ModelVersionManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2939340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e preparar dados\n",
    "texts, labels = trainer.load_data()\n",
    "train_dataset, val_dataset, test_dataset = trainer.prepare_data(texts, labels, seed_random_state=42)\n",
    "    \n",
    "print(f\"Dados preparados:\")\n",
    "print(f\"  Treino: {len(train_dataset)} amostras\")\n",
    "print(f\"  Validação: {len(val_dataset)} amostras\")\n",
    "print(f\"  Teste: {len(test_dataset)} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf24d9",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo\n",
    "trainer.train(train_dataset, val_dataset, BATCH_SIZE, EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39098f7",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar no conjunto de teste\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loss, test_accuracy, test_metrics = trainer.evaluate(test_loader)\n",
    "    \n",
    "print(f\"\\n=== Resultados Finais ===\")\n",
    "print(f\"Loss de teste: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Acurácia: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precisão: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"AUC-ROC: {test_metrics['auc_roc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar experimento\n",
    "experiment_path = version_manager.save_experiment(trainer, test_metrics, hyperparameters)\n",
    "    \n",
    "# Tentar promover para produção\n",
    "version_manager.promote_to_production(experiment_path)\n",
    "    \n",
    "print(f\"\\nExperimento salvo em: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788de7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the writer in TensorBoard\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
